---
title: "On the waning of forms - a corpus-based analysis of losers in language change (Part 1)"
author: "Martin Schweinberger"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  bookdown::html_document2: default
bibliography: bibliography.bib
link-citations: yes
---

```{r uq1, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("https://slcladal.github.io/images/isle6.png")
```


This document documents the analysis of the adjective amplifier *very* in New Zealand English. The analysis was performed with the aim of fitting a generalized-linear binomial logistic mixed-effects regression to explain which factors correlate with the use of *very* and to ascertain if 

1. *very* is decreasing across apparent time 

2. if the decrease of *very* is systematic or layered (which would imply that the use of very is stratified along linguistic and social lines) or unsystematic (which would imply that it correlates with few if any linguistic or social variables). 

# Introduction

We will now begin with the analysis. In a first step, the session is prepared by setting options and activating packages.

```{r isle6_1_01, message=F, warning=F}
# load packages
library(tidyverse)
library(tidyr)
library(knitr) 
library(kableExtra) 
library(DT)
library(usethis)
# set options
options(stringsAsFactors = F)
options(scipen = 999)
options(max.print=10000)
# specify paths
wscpath <- "D:\\Uni\\Korpora\\Original\\WSC"
biowscpath <- "D:\\Uni\\Korpora\\Original\\WSC\\DOC/PTPNTS.txt"
bioguidewsc <- "D:\\Uni\\Korpora\\Original\\WSC\\DOC/GUIDE.txt"
biolinkwsc <- "D:\\Uni\\Korpora\\Original\\WSC\\DOC/LINK.txt"
bioextractswsc <- "D:\\Uni\\Korpora\\Original\\WSC\\DOC/EXTRACTS.txt"
# define corpus files
corpusfiles = list.files(path = wscpath, pattern = ".TXT", all.files = T,
                         full.names = T, recursive = F, ignore.case = T, 
                         include.dirs = F)
```

# Data Processing

Load and process data

```{r isle6_1_02, message=F, warning=F}
# load and start processing corpus
wsc <- sapply(corpusfiles, function(x) {
  x <- scan(x, what = "char", sep = "", quote = "", quiet = T, skipNul = T)
  x <- gsub(" {2,}", " ", x)
  x <- stringr::str_trim(x, side = "both")
  x <- paste(x, sep = " ", collapse = " ")
  x <- strsplit(gsub("(<WSC#)", "~~~\\1", x), "~~~" )
  x <- as.vector(unlist(x))
  x <- x[2:length(x)]
} ) 
# extract number of text elements per file
idx <- sapply(wsc, function(x) length(x))
# extract file names
File <- names(wsc)
File <- rep(File, idx)
File <- gsub("D:\\Uni\\Korpora\\Original\\WSC/", "", File, fixed = T)
File <- gsub(".TXT", "", File)
# extract speakers
Speaker <- as.vector(unlist(sapply(wsc, function(x) {
  x <- gsub(">.*", "", x)
  x <- gsub(".*#", "", x)
  x <- gsub(".*:", "", x)
  })))
# extract raw text
Text <- as.vector(unlist(sapply(wsc, function(x) {
  x <- gsub("<[A-Z]{1,}#[A-Z]{1,}[0-9]{1,}:[0-9]{1,}:[A-Z]{1,}[0-9]{0,}>", "", x)
  x <- str_trim(x, side = "both")
  })))
# clean text
CleanText <- Text %>%
  stringr::str_replace_all("<O> {0,1}[[:alnum:] ]{0,}</O>", " ") %>%
  stringr::str_replace_all("<&> {0,1}[[:alnum:] ]{0,}[:punct:]{0,}[[:alnum:] ]{0,}</&>", " ") %>%
  stringr::str_replace_all("</{0,1}[:alnum:]{0,}>", " ") %>%
  stringr::str_replace_all("<[:punct:]{1,2}[:alnum:]{0,2}>", " ") %>%
  stringr::str_replace_all("<indig=[:alnum:]{1,}> {0,1}[[:alnum:] ]{0,}</indig=[:alnum:]{1,}>", " ") %>%
  stringr::str_replace_all(fixed("\""), " ") %>%
  stringr::str_replace_all("<foreign=[:alnum:]{1,}> {0,1}[[:alnum:] ]{0,}</foreign=[:alnum:]{1,}>", " ") %>%
  stringr::str_replace_all("<.*>", " ") %>%
  tolower() %>%
  stringr::str_replace_all(" {2,}", " ") %>%
  stringr::str_trim(side = "both")
# combine elements into a data frame 
wscdf <- data.frame(File, Speaker, Text, CleanText)
# remove problematic elements
#wscdf <- wscdf[-grep("\\[", wscdf$CleanText),]
# remove empty speech units
wscdf <- wscdf[wscdf$CleanText != "",]
nrow(wscdf)
```

Extract word counts

```{r isle6_1_03, message=F, warning=F}
# add wordcount
wordcount <- wscdf %>%
  dplyr::mutate(WordCount = str_count(CleanText, " ")+1) %>%
  dplyr::select(-Text, -CleanText) %>%
  dplyr::group_by(File, Speaker) %>%
  dplyr::summarise(Wordcount = sum(WordCount))
```


Save data to disc

```{r isle6_1_04, message=F, warning=F}
# save raw data to disc
write.table(wordcount, here::here("data", "wordcount.txt"), sep = "\t", row.names = F, col.names = T)
write.table(wscdf, here::here("data", "wscdf_raw.txt"), sep = "\t", row.names = F, col.names = T)
wscdf <- read.delim(here::here("data", "wscdf_raw.txt"), sep = "\t", header = T, skipNul = T)
```

## PoS-Tagging{-}

Split data

```{r isle6_1_05, message=F, warning=F}
# split data into smaller chunks
pos01 <- wscdf$CleanText[1:10000]
pos02 <- wscdf$CleanText[10001:20000]
pos03 <- wscdf$CleanText[20001:22000]
pos04 <- wscdf$CleanText[22001:23000]
pos05 <- wscdf$CleanText[23001:24000]
pos06 <- wscdf$CleanText[24001:25000]
pos07 <- wscdf$CleanText[25001:30000]
pos08 <- wscdf$CleanText[30001:40000]
pos09 <- wscdf$CleanText[40001:50000]
pos10 <- wscdf$CleanText[50001:60000]
pos11 <- wscdf$CleanText[60001:70000]
pos12 <- wscdf$CleanText[70001:80000]
pos13 <- wscdf$CleanText[80001:90000]
pos14 <- wscdf$CleanText[90001:100000]
pos15 <- wscdf$CleanText[100001:nrow(wscdf)]
```

Pos-tagging

```{r isle6_1_06, message=F, warning=F, eval = F}
source("D:\\R/POStagObject.R") # for pos-tagging objects in R
library(NLP)
library(openNLP)
# pos tagging data
wscpos01 <- POStag(object = pos01)
wscpos01 <- as.vector(unlist(wscpos01))
writeLines(wscpos01, con = here::here("data", "wscpos01.txt"), sep = "\n", useBytes = FALSE)
# chunk 2
wscpos02 <- POStag(object = pos02)
wscpos02 <- as.vector(unlist(wscpos02))
writeLines(wscpos02, con = here::here("data", "wscpos02.txt"), sep = "\n", useBytes = FALSE)
# chunk 03
wscpos03 <- POStag(object = pos03)
wscpos03 <- as.vector(unlist(wscpos03))
writeLines(wscpos03, con = here::here("data", "wscpos03.txt"), sep = "\n", useBytes = FALSE)
# chunk 04
wscpos04 <- POStag(object = pos04)
wscpos04 <- as.vector(unlist(wscpos04))
writeLines(wscpos04, con = here::here("data", "wscpos04.txt"), sep = "\n", useBytes = FALSE)
# chunk 05
wscpos05 <- POStag(object = pos05)
wscpos05 <- as.vector(unlist(wscpos05))
writeLines(wscpos05, con = here::here("data", "wscpos05.txt"), sep = "\n", useBytes = FALSE)
# chunk 06
wscpos06 <- POStag(object = pos06)
wscpos06 <- as.vector(unlist(wscpos06))
writeLines(wscpos06, con = here::here("data", "wscpos06.txt"), sep = "\n", useBytes = FALSE)
# chunk 07
wscpos07 <- POStag(object = pos07)
wscpos07 <- as.vector(unlist(wscpos07))
writeLines(wscpos07, con = here::here("data", "wscpos07.txt"), sep = "\n", useBytes = FALSE)
# chunk 08
wscpos08 <- POStag(object = pos08)
wscpos08 <- as.vector(unlist(wscpos08))
writeLines(wscpos08, con = here::here("data", "wscpos08.txt"), sep = "\n", useBytes = FALSE)
# chunk 09
wscpos09 <- POStag(object = pos09)
wscpos09 <- as.vector(unlist(wscpos09))
writeLines(wscpos09, con = here::here("data", "wscpos09.txt"), sep = "\n", useBytes = FALSE)
# chunk 10
wscpos10 <- POStag(object = pos10)
wscpos10 <- as.vector(unlist(wscpos10))
writeLines(wscpos10, con = here::here("data", "wscpos10.txt"), sep = "\n", useBytes = FALSE)
# chunk 11
wscpos11 <- POStag(object = pos11)
wscpos11 <- as.vector(unlist(wscpos11))
writeLines(wscpos11, con = here::here("data", "wscpos11.txt"), sep = "\n", useBytes = FALSE)
# chunk 12
wscpos12 <- POStag(object = pos12)
wscpos12 <- as.vector(unlist(wscpos12))
writeLines(wscpos12, con = here::here("data", "wscpos12.txt"), sep = "\n", useBytes = FALSE)
# chunk 13
wscpos13 <- POStag(object = pos13)
wscpos13 <- as.vector(unlist(wscpos13))
writeLines(wscpos13, con = here::here("data", "wscpos13.txt"), sep = "\n", useBytes = FALSE)
# chunk 14
wscpos14 <- POStag(object = pos14)
wscpos14 <- as.vector(unlist(wscpos14))
writeLines(wscpos14, con = here::here("data", "wscpos14.txt"), sep = "\n", useBytes = FALSE)
# chunk 15
wscpos15 <- POStag(object = pos15)
wscpos15 <- as.vector(unlist(wscpos15))
writeLines(wscpos15, con = here::here("data", "wscpos15.txt"), sep = "\n", useBytes = FALSE)
# list pos tagged elements
postag.files = c(here::here("data", "wscpos01.txt"), here::here("data", "wscpos02.txt"), 
                 here::here("data", "wscpos03.txt"), here::here("data", "wscpos04.txt"), 
                 here::here("data", "wscpos05.txt"), here::here("data", "wscpos06.txt"),  
                 here::here("data", "wscpos07.txt"), here::here("data", "wscpos08.txt"), 
                 here::here("data", "wscpos09.txt"), here::here("data", "wscpos10.txt"),
                 here::here("data", "wscpos11.txt"), here::here("data", "wscpos12.txt"),
                 here::here("data", "wscpos13.txt"), here::here("data", "wscpos14.txt"),
                 here::here("data", "wscpos15.txt"))
# load pos tagged elements
wscpos <- sapply(postag.files, function(x) {
  x <- scan(x, what = "char", sep = "\n", quote = "", quiet = T, skipNul = T)
  x <- gsub(" {2,}", " ", x)
  x <- str_trim(x, side = "both")
  x <- str_replace_all(x, fixed("\n"), " ")
})
# unlist pos tagged elements
wscdf$TextPOS <- unlist(wscpos)
```

Save data to disc

```{r isle6_1_07, message=F, warning=F}
# save data to disc
#write.table(wscdf, here::here("data", "wscdf_postagged.txt"), sep = "\t", row.names = F, col.names = T)
wscdf <- read.delim(here::here("data", "wscdf_postagged.txt"), sep = "\t", header = T, skipNul = T)
```

## Concordancing{-}

```{r isle6_1_08, message=F, warning=F}
# extract number of adjs per line
pstggd <- wscdf$TextPOS
lpstggd <- strsplit(pstggd, " ")
nlpstggd <- as.vector(unlist(sapply(lpstggd, function(x){
  x <- x[grep("[A-Z]{0,1}[a-z]{1,}\\/JJ[A-Z]{0,1}", x)]
  x <- length(x) } )))
rp <- nlpstggd
rp <- ifelse(rp == 0, 1, rp)
# detach dplyr package (clash with plyr)
detach("package:dplyr", unload=TRUE)
# load function for concordancing
source("D:\\R/ConcR_2.3_loadedfiles.R")
# set parameters for concordancing
pattern <- "[A-Z]{0,1}[a-z]{1,}\\/JJ[A-Z]{0,1}"
context <- 50
# extract all adjectives (concordance)
concjjwsc <- ConcR(wscdf$TextPOS, pattern, context, all.pre = FALSE)
# repeat rows in data frame as often as there are adjectives in it 
# (if 0 adj, repeat once)
wscadjdf <- wscdf[rep(seq(nrow(wscdf)), rp),]
# combine data sets
wscadjdf <- data.frame(1:nrow(wscadjdf), wscadjdf, concjjwsc)
# remove rows without Tokens
wscadjdf <- wscadjdf[is.na(wscadjdf$Token) == F,]
# add clean column names
colnames(wscadjdf)[1] <- "ID"
# clean adjectives
wscadjdf$Adjective <- str_replace_all(wscadjdf$Token, "/.*", "")
# add Variant column
wscadjdf$Variant <- gsub(".* ", "", str_trim(wscadjdf$PreContext, side = "both")) 
# inspect data
head(wscadjdf)
```



```{r isle6_1_09, message=F, warning=F}
# save raw data to disc
write.table(wscadjdf, here::here("data", "wscadjdf.txt"), sep = "\t", row.names = F, col.names = T)
wscadjdf <- read.delim(here::here("data", "wscadjdf.txt"), sep = "\t", header = T, skipNul = T)
```

## Process sociodemographic data{-}

```{r isle6_1_10, message=F, warning=F}
# load files
biowsc <- read.delim(biowscpath, sep = "\t", header = F, skipNul = T)
guidewsc <- read.delim(bioguidewsc, sep = "\t", header = F, skipNul = T)
linkwsc <- read.delim(biolinkwsc, sep = "\t", header = F, skipNul = T)
extractswsc <- read.delim(bioextractswsc, sep = "\t", header = F, skipNul = T)
# add column names
biowsc <- biowsc %>%
  dplyr::rename(Id = V1, Ethnicity = V2, Gender = V3, Age = V4, 
                Occupation = V5, Education = V6, L1 = V7) %>%
  mutate(Gender=replace(Gender, Gender=="F", "Woman")) %>%
  mutate(Gender=replace(Gender, Gender=="M", "Man"))
# add column names to guide
linkwsc <- linkwsc %>%
  dplyr::rename(File = V1, Id = V2, Speaker = V3, Words = V4)
# add column names to extracts
extractswsc <- extractswsc %>%
  dplyr::rename(File = V1, WordsFile = V2, Date = V3, Topic = V4, 
                ExtractMinutes = V5, Minutes = V6)
# join biowsc and linkwsc
wscspeakerinformation <- dplyr::left_join(biowsc, linkwsc, by = "Id")
wscspeakerinformation <- dplyr::left_join(wscspeakerinformation, extractswsc, by = "File")
# inspect
head(wscspeakerinformation)
```



```{r isle6_1_11, message=F, warning=F}
# add information about type
wscspeakerinformation <- wscspeakerinformation %>%
  dplyr::mutate(Type = File) %>%
  dplyr::mutate(Type = Type %>%
  str_remove_all("[0-9]{0,}") %>%
  str_replace_all("MSN", "BroadcastNews") %>%
  str_replace_all("MST", "BroadcastMonologue") %>%
  str_replace_all("MSW", "BroadcastWeather") %>%
  str_replace_all("MUC", "SportsCommentary") %>%
  str_replace_all("MUJ", "JudgesSummation") %>%
  str_replace_all("MUL", "Lecture") %>%
  str_replace_all("MUS", "TeacherMonologue") %>%
  str_replace_all("DPC", "Conversation") %>%
  str_replace_all("DPF", "TelephoneConversation") %>%
  str_replace_all("DPH", "OralHistoryInterview") %>%
  str_replace_all("DPP", "SocialDialectInterview") %>%
  str_replace_all("DGB", "RadioTalkback") %>%
  str_replace_all("DGI", "BroadcastInterview") %>%
  str_replace_all("DGU", "ParliamentaryDebate") %>%
  str_replace_all("DGZ", "TransactionsAndMeetings"))
# inspect speaker information
head(wscspeakerinformation); nrow(wscspeakerinformation)
```



```{r isle6_1_12, message=F, warning=F}
# save raw data to disc
write.table(wscspeakerinformation, here::here("data", "wscspeakerinformation.txt"), sep = "\t", 
            row.names = F, col.names = T)
wscspeakerinformation <- read.delim(here::here("data", "wscspeakerinformation.txt"), sep = "\t", 
                                    header = T, skipNul = T)
wscadjdf <- read.delim(here::here("data", "wscadjdf.txt"), sep = "\t", header = T, skipNul = T)
```

## Combine data with biodata{-}

```{r isle6_1_13, message=F, warning=F}
# join wscadjdf and wscspeakerinformation
wsc <- join(wscadjdf, wscspeakerinformation, by = c("File", "Speaker"))
```

## Frequency{-}

```{r isle6_1_14, message=F, warning=F}
# code freq of adj type by date category
frqadjtb <- table(wsc$Age, wsc$Adjective)
relfreqadjtb <- round(prop.table(frqadjtb, margin = 1)*100, 5)
relfreqadjdf <- as.data.frame(relfreqadjtb)
colnames(relfreqadjdf)[1:2] <- c("Age", "Adjective")
# add freq by date to data
wscadjdf <- join(wsc, relfreqadjdf, by=c("Age", "Adjective"))
# relabel Freq
colnames(wscadjdf)[which(colnames(wscadjdf) == "Freq")] <- "Frequency"
# inspect data
head(wscadjdf)
```

Save data to disc

```{r isle6_1_15, message=F, warning=F}
# save raw data to disc
write.table(wscadjdf, here::here("data", "wscadjdf_spk.txt"), sep = "\t", row.names = F, col.names = T)
wscadjdf <- read.delim(here::here("data", "wscadjdf_spk.txt"), sep = "\t", header = T, skipNul = T)
```

## Check Amplifiers{-}

not included: *quite*, *too*, and *well*.

```{r isle6_1_16, message=F, warning=F}
# define amplifiers 
amplifiers <- c("absolutely", "actually", "aggressively", "amazingly", "appallingly", "awful", 
                "awfully", "badly", "bloody", "certainly", "clearly", "complete", "dead", "completely", 
                "considerably", "crazy", "decidedly", "definitely", "distinctly", "dreadfully", 
                "enormously", "entirely", "especially", "exactly", "exceedingly", "exceptionally", 
                "excruciatingly", "extraordinarily", "extremely", "fiercely", "firmly", "frightfully", 
                "fucking", "fully", "genuinely", "greatly", "grossly", "heavily", "highly", "hopelessly",
                "horrendously", "hugely", "immediately", "immensely", "incredibly", "infinitely", 
                "intensely", "irrevocably", "mad", "mega", "mighty", "most", "much", "obviously", 
                "openly", "overwhelmingly", "particularly", "perfectly", "plenty", "positively", 
                "precisely", "pretty", "profoundly", "purely", "real", "really", "remarkably", "seriously",
                "shocking", "significant", "significantly", "so", "specially", "specifically", "strikingly", 
                "strongly", "substantially", "super", "surely", "terribly", "terrifically", "total", 
                "totally", "traditionally", "true", "truly", "ultra", "utterly", "very", "viciously", 
                "wholly", "wicked", "wildly")
```

## Syntactic Function{-}

```{r isle6_1_17, message=F, warning=F}
# determine function
Function1 <- wscadjdf$PostContext %>%
  stringr::str_trim(side = "both") %>%
  tolower() %>%
  stringr::str_replace_all(" {2,}", " ") %>%
  stringr::str_replace_all(" .*", "") %>%
  stringr::str_replace_all(" .*", "") %>%
  stringr::str_replace_all(".*/n.*", "Attributive") %>%
  stringr::str_replace_all(fixed("/."), "PUNCT") %>%
  stringr::str_replace_all(".*PUNCT.*", "Attributive")
Function2 <-  wscadjdf$PostContext %>%
  stringr::str_trim(side = "both") %>%
  tolower() %>%
  stringr::str_replace(" ", "") %>%
  stringr::str_replace_all(" .*", "") %>%
  stringr::str_replace_all(".*/n.*", "Attributive") %>%
  stringr::str_replace_all(fixed("/."), "PUNCT") %>%
  stringr::str_replace_all(".*PUNCT.*", "Attributive")
functiontb <- cbind(Function1, Function2)
wscadjdf$Function <-  as.vector(unlist(apply(functiontb, 1, function(x){
  x <-   ifelse(x[1] == "" | x[1] == "Attributive" | x[2] == "Attributive", "Attributive", "Predicative")
})))
# shorten post Context
wscadjdf$PostContext <- substr(wscadjdf$PostContext, 1, ifelse((nchar(wscadjdf$PostContext)+25) <25, max(nchar(wscadjdf$PostContext)), 25))
# pre Context
wscadjdf$PreContext <- str_trim(wscadjdf$PreContext, side = "both")
wscadjdf$PreContextLong <- wscadjdf$PreContext
wscadjdf$PreContextLong <- substr(wscadjdf$PreContextLong, ifelse(nchar(wscadjdf$PreContextLong)-25 <=0, 1, 
                                                              nchar(wscadjdf$PreContextLong)-25), nchar(wscadjdf$PreContextLong))
wscadjdf$PreContext <- gsub(".* ", "", wscadjdf$PreContext)
# amplifier variant
wscadjdf$PreContext <- gsub("\\/.*", "", wscadjdf$PreContext)
wscadjdf$Variant <- ifelse(wscadjdf$PreContext %in% amplifiers, wscadjdf$PreContext, "0")
# amplified y/n
wscadjdf$Amplified <- ifelse(wscadjdf$Variant == "0", 0, 1) 
# adjective
wscadjdf$Adjective <- tolower(wscadjdf$Adjective)
# inspect data
nrow(wscadjdf); head(wscadjdf); table(wscadjdf$Variant)
```

## Code Priming{-}

```{r isle6_1_18, message=F, warning=F}
# code priming
prim1 <- c(rep(0, 1), wscadjdf$Variant[1:length(wscadjdf$Variant)-1])
prim2 <- c(rep(0, 2), wscadjdf$Variant[1:(length(wscadjdf$Variant)-2)])
prim3 <- c(rep(0, 3), wscadjdf$Variant[1:(length(wscadjdf$Variant)-3)])
primtb <- cbind(wscadjdf$Variant, prim1, prim2, prim3)

wscadjdf$Priming <- as.vector(unlist(apply(primtb, 1, function(x){
  x <- ifelse(x[1]== "0" , "NoPrime",
              ifelse(x[1] == x[2] | x[1] == x[3] | x[1] == x[4], "Prime", "NoPrime"))
})))
# define forms that require removal
sups <- c(".*most.*", ".*more.*") 
negs <- c(".*not.*", ".*never.*", ".*n't.*")
downtoners <- c(".*sort/.*", ".*kind/.*", ".* bit/.*", ".*somewhat.*", ".*fairly.*", 
                ".*rather.*", ".*reasonably.*", ".*slightly.*", ".*comparatively.*", ".*semi.*", 
                ".*relatively.*", ".*little.*", ".*somehow.*", ".*almost.*", ".*partly.*", 
                ".*hardly.*", ".* less.*", ".*barely.*", ".* just/.*")
specialforms <- c(".* too.*", ".*quite.*")
PostContextdowntoners <- c(".*enough.*")
nonpropadj <- c("only", "much", "many", "cheaper", "cheaperr", "bests", "larger",
                "bst", "better", "bigger")
# check length of dataset
str(wscadjdf); head(wscadjdf); nrow(wscadjdf)
```

## Remove negated forms et al.{-}

```{r isle6_1_19, message=F, warning=F}
# find items to be removed
supsidx <- unique(grep(paste(sups,collapse="|"), wscadjdf$PreContextLong, value=F))
negsidx <- unique(grep(paste(negs,collapse="|"), wscadjdf$PreContextLong, value=F))
downtonersidx <- unique(grep(paste(downtoners,collapse="|"), wscadjdf$PreContextLong, value=F))
specialformsidx <- unique(grep(paste(specialforms,collapse="|"), wscadjdf$PreContextLong, value=F))
PostContextdowntonersidx <- unique(grep(paste(PostContextdowntoners,collapse="|"), wscadjdf$PostContext, value=F))
nonpropadjidx <- unique(grep(paste(nonpropadj,collapse="|"), wscadjdf$Adjective, value=F))
# combine indices
idxs <- unique(c(supsidx, negsidx, downtonersidx, specialformsidx, PostContextdowntonersidx, nonpropadjidx))
# remove forms that require removal
wscadjdf <- wscadjdf[-idxs,]
# remove empty values
wscadjdf <- wscadjdf[!wscadjdf$Variant == "", ]
```



```{r isle6_1_20, message=F, warning=F}
# save raw data to disc
write.table(wscadjdf, here::here("data", "wscadjdf_wo_neg.txt"), sep = "\t", row.names = F)
wscadjdf <- read.delim(here::here("data", "wscadjdf_wo_neg.txt"), sep = "\t", header = T, skipNul = T)
```

## Remove lexicalizations 

Remove variants that were not intensified by at least 2 different amplifier variants

> NOT EXECTUTED!

```{r isle6_1_21, message=F, warning=F, eval = F}
nrow(wscadjdf)
# remove items that were not intensified by a minimum of 2 intensifier variants
pintadjtb <- table(wscadjdf$Adjective, wscadjdf$Variant)
#pintadjtb <- pintadjtb[2:nrow(pintadjtb),]
pintadjtb <- pintadjtb[,2:ncol(pintadjtb)]
pintadjtb2 <- apply(pintadjtb, 1, function(x){
  x <- ifelse(x > 1, 1, x)})
pintadjtb3 <- colSums(pintadjtb2)
pintadjschildes <- names(pintadjtb3)[which(pintadjtb3 >= 2)]
wscadjdf <- wscadjdf[wscadjdf$Adjective %in% pintadjschildes, ]
nrow(wscadjdf)
```

## Remove misspelled forms {-}

```{r isle6_1_22, message=F, warning=F, eval = F}
# inspect adjectives
names(table(wscadjdf$Adjective))
```

Remove words not in dictionary or words that consist only of 1 or 2 characters

```{r isle6_1_23, message=F, warning=F}
# correct spelling
library(hunspell)
wscadjdf <- wscadjdf %>%
  dplyr::filter(hunspell_check(Adjective)) %>%
  dplyr::filter(nchar(Adjective) > 2)
# inspect
head(wscadjdf)
```



## Remove strange forms{-}

```{r isle6_1_24, message=F, warning=F}
# inspect adjectives
names(table(wscadjdf$Adjective))
```

Remove mistagged items

```{r isle6_1_25, message=F, warning=F}
library(udpipe)
#model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(file = here::here("english-ewt-ud-2.5-191206.udpipe"))
udpipe_adjectives <- udpipe_annotate(udmodel_english, wscadjdf$Adjective) %>%
  data.frame() %>%
  dplyr::filter(xpos == "JJ") %>%
  dplyr::pull(sentence)
# inspect
head(udpipe_adjectives)
```

```{r isle6_1_26, message=F, warning=F}
nrow(wscadjdf)
wscadjdf <-wscadjdf %>%
  dplyr::filter(Adjective %in% udpipe_adjectives)
# inspect
nrow(wscadjdf)
```

Remove adjectives that were never amplified


```{r isle6_1_27, message=F, warning=F}
amp_adjs <- wscadjdf %>%
  dplyr::filter(Amplified == 1) %>%
  dplyr::pull(Adjective) %>%
  table() %>%
  names()
# inspect
head(amp_adjs)
```
```{r isle6_1_28, message=F, warning=F}
nrow(wscadjdf)
wscadjdf <- wscadjdf %>%
  dplyr::filter(Adjective %in% amp_adjs)
nrow(wscadjdf)
```


Save data to disc

```{r isle6_1_29, message=F, warning=F}
# save raw data to disc
write.table(wscadjdf, here::here("data", "wscadjdf_clean.txt"), sep = "\t", row.names = F)
wscadjdf <- read.delim(here::here("data", "wscadjdf_clean.txt"), sep = "\t", header = T, skipNul = T)
```

## Gradability{-}

```{r isle6_1_30, message=F, warning=F}
# code gradability
# load Gradability data (derived from COCA)
gradability <- read.delim(here::here("data", "Gradability.txt"), sep = "\t", header = T, quote = "", skipNul = T)
wscadjdf$Gradability <- ifelse(wscadjdf$Adjective %in% gradability$Adjective, gradability$Beta, 1)
# inspect data
nrow(wscadjdf); head(wscadjdf)
```

## Semantic Classification{-}

Add semantic types [@tagliamonte2008intensifiers, @dixon1977adjectives]

* dimension = semdim (e.g. big, large, little, small, long, short, wide, narrow, thick)

* difficulty = semdif (e.g. difficult, simple)

* physical property = (e.g. hard, soft, heavy, light, rough, smooth, hot, sweet)

* color = semcol (e.g. black, white, red)

* human propensity: semhup (e.g. jealous, happy, kind, clever, generous, gay, rude)

* age = semage (e.g. new, young, old) 

* value (e.g. good, bad, proper, perfect, excellent, delicious, poor), 

* speed Speed (fast, quick, slow)

* position (e.g. right, left, near, far)

* other

```{r isle6_1_31, message=F, warning=F}
# load data
code1 <- read.delim(here::here("data", "semcodecg1.txt"), sep = "\t", header = T, skipNul = T)
code2 <- read.delim(here::here("data", "semcodedjm1.txt"), sep = "\t", header = T, skipNul = T)
code3 <- read.delim(here::here("data", "semcodedl1.txt"), sep = "\t", header = T, skipNul = T)
code4 <- read.delim(here::here("data", "semcodedm1.txt"), sep = "\t", header = T, skipNul = T)
code5 <- read.delim(here::here("data", "semcodedma1.txt"), sep = "\t", header = T, skipNul = T)
code6 <- read.delim(here::here("data", "semcodedv1.txt"), sep = "\t", header = T, skipNul = T)
code7 <- read.delim(here::here("data", "semcodedjw1.txt"), sep = "\t", header = T, skipNul = T)
# order data sets
code1 <- code1[order(code1$Id),]
code2 <- code2[order(code2$Id),]
code3 <- code3[order(code3$Id),]
code4 <- code4[order(code4$Id),]
code5 <- code5[order(code5$Id),]
code6 <- code6[order(code6$Id),]
code7 <- code6[order(code7$Id),]
# repair adjectives in code1
code3$Adjective <- code1$Adjective
# combine tables
semcode <- rbind(code1, code2, code3, code4, code5, code6, code7)
# convert coding into numeric values
semcode[,3:12] <- t(apply(semcode[,3:12], 1, function(x) {
  x <- ifelse(x == "" | x == "?"| is.na(x) == T, 0, 1)}))
# convert into data frame
semcode <- as.data.frame(semcode)
# add column names
colnames(semcode)[3:12] <- c("Dimension", "Difficulty", "PhysicalProperty", "Color",
                             "HumanPropensity", "Age", "Value", "Speed", "Position", 
                             "Other")
```


```{r isle6_1_32, message=F, warning=F}
# load package
library(dplyr)
AdjectiveSemantics <- semcode %>%
  dplyr::group_by(Adjective) %>%
  na.omit() %>%
  dplyr::summarize(Dimension = sum(Dimension), 
                   Difficulty = sum(Difficulty),
                   PhysicalProperty = sum(PhysicalProperty), 
                   Color = sum(Color),
                   HumanPropensity = sum(HumanPropensity), 
                   Age = sum(Age),
                   Value = sum(Value), 
                   Speed = sum(Speed),
                   Position = sum(Position), 
                   Other = sum(Other)) %>%
  dplyr::mutate(OverallScore = rowSums(.[,2:11])) %>%
  dplyr::mutate(Maximum = do.call(pmax, (.[,2:11]))) %>%
  dplyr::mutate(Certainty = Maximum/OverallScore*100)
AdjectiveSemantics <- AdjectiveSemantics %>%
  dplyr::mutate(Category = colnames(AdjectiveSemantics[2:11])[apply(AdjectiveSemantics[2:11],1,which.max)])
# inspect interrater reliability
summary(AdjectiveSemantics$Certainty)
```

```{r isle6_1_33, message=F, warning=F}
Age <- AdjectiveSemantics %>%
  dplyr::filter(Age != 0) %>%
  dplyr::pull(Adjective)
Color <- AdjectiveSemantics %>%
  dplyr::filter(Color != 0) %>%
  dplyr::pull(Adjective)
Difficulty <- AdjectiveSemantics %>%
  dplyr::filter(Difficulty != 0) %>%
  dplyr::pull(Adjective)
Dimension <- AdjectiveSemantics %>%
  dplyr::filter(Dimension != 0) %>%
  dplyr::pull(Adjective)
HumanPropensity <- AdjectiveSemantics %>%
  dplyr::filter(HumanPropensity != 0) %>%
  dplyr::pull(Adjective)
PhysicalProperty <- AdjectiveSemantics %>%
  dplyr::filter(PhysicalProperty != 0) %>%
  dplyr::pull(Adjective)
Position <- AdjectiveSemantics %>%
  dplyr::filter(Position != 0) %>%
  dplyr::pull(Adjective)
Speed <- AdjectiveSemantics %>%
  dplyr::filter(Speed != 0) %>%
  dplyr::pull(Adjective)
Value <- AdjectiveSemantics %>%
  dplyr::filter(Value != 0) %>%
  dplyr::pull(Adjective)
# add semantic category to data
wscadjdf <- wscadjdf %>%
  dplyr::mutate(SemanticCategory = dplyr::case_when(Adjective %in% Age ~ "Age",
                                                    Adjective %in% Color ~ "Color",
                                                    Adjective %in% Difficulty ~ "Difficulty",
                                                    Adjective %in% Dimension ~ "Dimension",
                                                    Adjective %in% HumanPropensity ~ "HumanPropensity",
                                                    Adjective %in% PhysicalProperty ~ "PhysicalProperty",
                                                    Adjective %in% Position ~ "Position",
                                                    Adjective %in% Speed ~ "Speed",
                                                    Adjective %in% Value ~ "Value", 
                                                    TRUE ~ "Other"))
# table sem class of tokens
table(wscadjdf$SemanticCategory)
```


## Sentiment Analysis{-}

```{r isle6_1_34, message=F, warning=F}
# positive adjectives
pos_adj <- tidytext::get_sentiments("nrc") %>%
  dplyr::filter(sentiment == "positive") %>%
  dplyr::pull(word)
# negative adjectives
neg_adj <- tidytext::get_sentiments("nrc") %>%
  dplyr::filter(sentiment == "negative") %>%
  dplyr::pull(word)
# annotate emotionality
wscadjdf <- wscadjdf %>%
  dplyr::mutate(Emotionality = dplyr::case_when(Adjective %in% pos_adj ~ "positive",
                                                Adjective %in% neg_adj ~ "negative",
                                                TRUE ~ "neutral")) %>%
  dplyr::mutate(Emotionality = factor(Emotionality, levels = c("neutral", "negative", "positive")))
# inspect
head(wscadjdf)
```

## Correct errors{-}

```{r isle6_1_35, message=F, warning=F}
wscadjdf %>%
  dplyr::filter(Variant == "so") %>%
  dplyr::group_by(Variant, Function) %>%
  dplyr::summarize(Frequency = n())
```


```{r isle6_1_36, message=F, warning=F}
wscadjdf <- wscadjdf %>%
  dplyr::mutate(Function = ifelse(Variant == "so" & Function == "Attributive", "Predicative", Function))
```

```{r isle6_1_37, message=F, warning=F}
wscadjdf %>%
  dplyr::filter(Variant == "so") %>%
  dplyr::group_by(Variant, Function) %>%
  dplyr::summarize(Frequency = n())
```

```{r isle6_1_38, message=F, warning=F}
# inspect data
wscadjdf %>%
  head(10) %>%
  kable(caption = "First 10 rows of wscadjdf.") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                            full_width = F)
```


```{r isle6_1_39, message=F, warning=F}
# save raw data to disc
write.table(wscadjdf, here::here("data", "wsc_fullclean.txt"), sep = "\t", row.names = F, col.names = T)
```


# Citation & Session Info {-}

Schweinberger, Martin. `r format(Sys.time(), '%Y')`. *On the waning of forms - a corpus-based analysis of losers in language change (Part 1)*. Brisbane: The University of Queensland. url: https://slcladal.github.io/isle6verynze1.html (Version `r format(Sys.time(), '%Y.%m.%d')`).

```
@manual{schweinberger`r format(Sys.time(), '%Y')`isle6verynze1,
  author = {Schweinberger, Martin},
  title = {On the waning of forms - a corpus-based analysis of losers in language change (Part 1)},
  note = {https://slcladal.github.io/isle6verynze1.html},
  year = {`r format(Sys.time(), '%Y')`},
  organization = {The University of Queensland, School of Languages and Cultures},
  address = {Brisbane},
  edition = {`r format(Sys.time(), '%Y.%m.%d')`}
}
```

```{r isle6_1_40, message=F, warning=F}
sessionInfo()
```


***

[Back to top](#introduction)


***

# References {-}

